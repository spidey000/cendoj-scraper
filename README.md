# ğŸ‡ªğŸ‡¸ Cendoj PDF Discovery System

**Sistema industrial de descubrimiento exhaustivo de enlaces PDF del Cendoj**

[![Status](https://img.shields.io/badge/status-active-brightgreen.svg)](https://github.com/spidey000/cendoj-scraper)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ“– Tabla de Contenidos

- [DescripciÃ³n](#descripciÃ³n)
- [CaracterÃ­sticas](#caracterÃ­sticas)
- [Arquitectura](#arquitectura)
- [InstalaciÃ³n](#instalaciÃ³n)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [Uso del CLI](#uso-del-cli)
- [Esquema de Base de Datos](#esquema-de-base-de-datos)
- [Anti-Blocking Stack](#anti-blocking-stack)
- [Rendimiento Estimado](#rendimiento-estimado)
- [Troubleshooting](#troubleshooting)
- [Desarrollo](#desarrollo)
- [Licencia](#licencia)

---

## ğŸ“ DescripciÃ³n

El **Cendoj PDF Discovery System** es una herramienta de cÃ³digo abierto diseÃ±ada para **descubrir exhaustivamente todos los enlaces de documentos PDF** del [Centro de DocumentaciÃ³n Judicial (Cendoj)](https://www.cendoj.es) de EspaÃ±a.

### **PropÃ³sito Principal**

> **Objetivo**: Encontrar el **100% de los enlaces PDF** disponibles en el Cendoj, sin ser bloqueado por el servidor.

Este proyecto **ya no se centra en descargar archivos**, sino en **descubrir y catalogar** todos los enlaces existentes. El sistema:

- âœ… Realiza **crawling profundo** (Breadth-First Search) sin lÃ­mites de profundidad
- âœ… **Evita bloqueos** mediante rotaciÃ³n automÃ¡tica de proxies, user-agents y rate limiting adaptativo
- âœ… Detecta y **maneja CAPTCHAs** automÃ¡ticamente (pausa para resoluciÃ³n manual)
- âœ… **Registra todo** en una base de datos SQLite con tracking completo
- âœ… **Resumible**: puedes interrumpir con `Ctrl+C` y continuar despuÃ©s
- âœ… **Escalable**: puede correr durante semanas de forma autÃ³noma

### **Casos de Uso**

- InvestigaciÃ³n acadÃ©mica del derecho espaÃ±ol
- CreaciÃ³n de archivos legales distribuidos
- AnÃ¡lisis de jurisprudencia a gran escala
- Proyectos de justicia abierta
- Backup legal de sentencias

---

## âœ¨ CaracterÃ­sticas

### **ğŸ” Sistema de Discovery**

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| **Modo Shallow** | Extrae solo de tablas HTML (rÃ¡pido, superficie) |
| **Modo Deep** | BFS con profundidad limitada (balanceado) |
| **Modo Full** | **BFS sin lÃ­mites** - encuentra TODOS los enlaces |
| **ExtracciÃ³n multi-mÃ©todo** | CSS selectors + Regex + Script scanning |
| **DeduplicaciÃ³n inteligente** | URLs normalizadas, evita duplicados |
| **ValidaciÃ³n opcional** | HEAD request para verificar accesibilidad |
| **Sesiones persistentes** | Resume desde Ãºltimo punto automÃ¡ticamente |

### **ğŸ›¡ï¸ Anti-Blocking Multi-Capa**

1. **Proxy Manager**: Pool de 3000+ proxies pÃºblicos rotativos
2. **User-Agent Pool**: 50+ fingerprints de navegadores reales
3. **Rate Limiter Adaptativo**: Se ajusta automÃ¡ticamente ante 429s
4. **Behavior Simulator**: Retrasos y movimientos humanos (opcional)
5. **CAPTCHA Handler**: DetecciÃ³n + screenshot + pausa manual
6. **Fingerprint Spoofing**: Enmascara automatizaciÃ³n (WebGL, Canvas, WebRTC)

### **ğŸ“Š Base de Datos Completa**

```sql
-- Enlaces PDF descubiertos
pdf_links (id, url, normalized_url, status, discovered_at, ...)

-- Sesiones de discovery (resumible)
discovery_sessions (id, mode, status, pages_visited, ...)

-- Salud de proxies
proxy_health (proxy_url, score, success_rate, ...)
```

### **ğŸ”§ CLI Completo**

```bash
python cli.py discover      # Iniciar discovery
python cli.py stats         # Ver estadÃ­sticas
python cli.py export        # Exportar enlaces
python cli.py proxies       # Estado del pool
python cli.py sessions      # Sesiones recientes
```

### **ğŸ“ˆ Monitoreo en Tiempo Real**

- Logs estructurados (JSON opcional)
- MÃ©tricas de success rate por proxy
- Tracking de CAPTCHAs detectados
- EstadÃ­sticas de pÃ¡ginas/segundo
- Alertas automÃ¡ticas (archivos)

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLI Interface                           â”‚
â”‚                  (cli.py - Click commands)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    DiscoveryScanner                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚Navigator â”‚  â”‚DeepCrawlerâ”‚  â”‚ProxyManagerâ”‚  â”‚UAPool    â”‚  â”‚
â”‚   â”‚(shallow) â”‚  â”‚(BFS)     â”‚  â”‚(3000+ IPs) â”‚  â”‚(50+ UAs) â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Anti-Blocking Stack                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚RateLimiter â”‚  â”‚CaptchaHandlerâ”‚  â”‚BehaviorSim   â”‚         â”‚
â”‚  â”‚(adaptive)  â”‚  â”‚(auto-detect) â”‚  â”‚(human-like)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Persistence Layer                               â”‚
â”‚            SQLite (pdf_links, sessions, proxy_health)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flujo de Discovery Full**:

```
1. InicializaciÃ³n
   â”œâ”€ Cargar configuraciÃ³n
   â”œâ”€ Inicializar DB
   â”œâ”€ Fetch proxies (Proxifly + ProxyScrape)
   â”œâ”€ Validar proxies (test concurrente)
   â”œâ”€ Cargar User-Agents
   â””â”€ Crear sesiÃ³n en DB

2. Crawl BFS
   â”œâ”€ URL semilla â†’ Browser (con proxy + UA)
   â”œâ”€ Extraer PDFs (CSS + Regex + Scripts)
   â”œâ”€ Guardar en DB (deduplicaciÃ³n)
   â”œâ”€ Validar URLs (HEAD request opcional)
   â”œâ”€ Extraer enlaces internos
   â”œâ”€ AÃ±adir a cola BFS
   â”œâ”€ Rotar proxy/UA
   â”œâ”€ Aplicar rate limiting
   â”œâ”€ Detectar CAPTCHA (pausar si se encuentra)
   â””â”€ Persistir estado cada 100 pÃ¡ginas

3. FinalizaciÃ³n
   â”œâ”€ Guardar estado final
   â”œâ”€ Actualizar sesiÃ³n DB
   â””â”€ Generar reporte
```

---

## ğŸš€ InstalaciÃ³n

### **Requisitos del Sistema**

- Python 3.8+
- 4GB RAM mÃ­nimo (8GB recomendado)
- Espacio en disco: 10GB+ (DB puede crecer a Millions de registros)
- ConexiÃ³n a internet estable

### **Pasos**

```bash
# 1. Clonar repositorio
git clone https://github.com/spidey000/cendoj-scraper.git
cd cendoj-scraper

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 3. Instalar dependencias
pip install -r requirements.txt
pip install -U playwright
playwright install chromium

# 4. Configurar directorios
mkdir -p data/sessions data/backups logs

# 5. Setup inicial de proxies (una sola vez)
python scripts/setup_proxies.py

# 6. Actualizar user agents (opcional pero recomendado)
python scripts/harvest_agents.py
```

---

## âš™ï¸ ConfiguraciÃ³n

### **Archivos de ConfiguraciÃ³n**

| Archivo | PropÃ³sito |
|---------|-----------|
| `config/sites.yaml` | ConfiguraciÃ³n principal (sites, discovery, anti-blocking) |
| `config/user_agents.txt` | Pool de user agents (uno por lÃ­nea) |
| `.env` | Variables de entorno (opcional, para overrides) |

### **ConfiguraciÃ³n Esencial (`config/sites.yaml`)**

```yaml
# ============================================
# DISCOVERY CONFIGURATION
# ============================================
discovery:
  mode: "full"              # shallow|deep|full
  max_depth: 0              # 0 = unlimited
  validate_on_discovery: true  # HEAD request despuÃ©s de encontrar PDF
  deduplicate: true

# ============================================
# ANTI-BLOCKING STACK
# ============================================
anti_blocking:
  proxy:
    enabled: true
    sources: ["proxifly", "proxyscraper"]
    refresh_hours: 6
    min_anonymity: "elite"
    test_before_use: true
    rotate_per_request: true

  user_agent:
    pool_file: "config/user_agents.txt"
    rotate_per_session: true

  rate_limiting:
    requests_per_minute: 20   # Ajustar segÃºn respuesta del servidor
    backoff_on_429: true

  captcha:
    auto_detect: true
    pause_on_captcha: true   # Pausar para resoluciÃ³n manual

# ============================================
# BROWSER
# ============================================
browser:
  headless: true
  stealth: true
  timeout: 60000  # 60s para deep crawl
```

### **Overrides con Variables de Entorno**

Puedes sobreescribir cualquier configuraciÃ³n con variables de entorno:

```bash
export CENDOJ__DISCOVERY__MODE=full
export CENDOJ__ANTI_BLOCKING__PROXY__ENABLED=true
export CENDOJ__RATE_LIMITING__REQUESTS_PER_MINUTE=30
```

Formato: `CENDOJ__SECTION__SUBSECTION__KEY=value`

---

## ğŸ’» Uso del CLI

### **Comandos Principales**

#### **1. Discover (Descubrir PDFs)**

```bash
# Modo FULL (recomendado) - BFS sin lÃ­mites
python cli.py discover --mode full --validate

# Modo DEEP - BFS con lÃ­mite de profundidad
python cli.py discover --mode deep --validate --limit 1000

# Modo SHALLOW - Solo tablas (mÃ¡s rÃ¡pido, menos completo)
python cli.py discover --mode shallow --limit 500

# Reanudar sesiÃ³n interrumpida
python cli.py discover --mode full --resume

# Sin validaciÃ³n (solo discover, mÃ¡s rÃ¡pido)
python cli.py discover --mode full --no-validate

# Con lÃ­mite de pÃ¡ginas (debug/testing)
python cli.py discover --mode full --limit 100
```

**Opciones**:
- `--mode`: Modo de discovery (shallow|deep|full)
- `--validate` / `--no-validate`: Realizar HEAD request para verificar URLs
- `--resume`: Reanudar Ãºltima sesiÃ³n interrumpida
- `--limit N`: LÃ­mite de pÃ¡ginas a visitar (0 = sin lÃ­mite)

#### **2. EstadÃ­sticas**

```bash
python cli.py stats
```

Salida de ejemplo:
```
ğŸ“Š ESTADÃSTICAS DE DISCOVERY
========================================
ğŸ“„ Enlaces PDF:
   Total descubiertos: 45,231
   Accesibles: 41,892 (92.6%)
   Rotos: 2,145 (4.7%)
   Bloqueados: 1,194 (2.6%)
   Validados: 43,000 (95.1%)

ğŸ”„ Sesiones:
   Total: 3
   Completadas: 2
   Fallidas: 0
   En ejecuciÃ³n: 1

ğŸ“… Ãšltima sesiÃ³n:
   ID: abc123def456
   Modo: full
   Estado: running
   PÃ¡ginas visitadas: 15,234
   Enlaces encontrados: 45,231
```

#### **3. Exportar Enlaces**

```bash
# Exportar todos los enlaces accesibles a CSV
python cli.py export --status accessible --output accessible_links.csv

# Exportar solo los rotos a JSON
python cli.py export --status broken --output broken.json

# Exportar todos los descubiertos (texto plano, un URL por lÃ­nea)
python cli.py export --status discovered --output all_urls.txt --limit 10000
```

**Formatos soportados**: `.csv`, `.json`, `.txt`

#### **4. GestiÃ³n de Proxies**

```bash
# Ver estado del pool
python cli.py proxies

# Refresh manual del pool
python scripts/setup_proxies.py

# Test de performance (stress test)
python scripts/test_proxies.py
```

#### **5. Sesiones**

```bash
# Listar sesiones recientes
python cli.py sessions

# Ver detalles de una sesiÃ³n especÃ­fica (en DB directly)
sqlite3 data/cendoj.db "SELECT * FROM discovery_sessions ORDER BY start_time DESC LIMIT 5;"
```

---

## ğŸ“Š Esquema de Base de Datos

### **Tablas Principales**

#### **`pdf_links`** - Enlaces PDF descubiertos

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `id` | INTEGER PK | ID interno |
| `url` | TEXT | URL original descubierta |
| `normalized_url` | TEXT UNIQUE | URL normalizada (sin query params irrelevantes) |
| `source_url` | TEXT | PÃ¡gina donde se encontrÃ³ el enlace |
| `discovery_session_id` | TEXT FK | SesiÃ³n que lo descubriÃ³ |
| `discovered_at` | DATETIME | Timestamp de descubrimiento |
| `validated_at` | DATETIME | Timestamp de validaciÃ³n (HEAD) |
| `status` | TEXT | discovered\|validated\|accessible\|broken\|blocked\|downloaded |
| `http_status` | INTEGER | CÃ³digo HTTP de validaciÃ³n |
| `content_type` | TEXT | MIME type |
| `content_length` | INTEGER | TamaÃ±o en bytes |
| `final_url` | TEXT | URL despuÃ©s de redirects |
| `redirect_count` | INTEGER | NÃºmero de redirects |
| `validation_error` | TEXT | Error si fallÃ³ validaciÃ³n |
| `extraction_method` | TEXT | css\|regex\|script_scan\|sitemap |
| `extraction_confidence` | FLOAT | Confianza 0-1 |
| `metadata` | JSON | `{"depth": 2, "site_key": "cendoj", ...}` |

**Ãndices**:
- `idx_pdf_links_normalized_url` (Ãºnico)
- `idx_pdf_links_discovery_session`
- `idx_pdf_links_status`
- `idx_pdf_links_discovered_at`

#### **`discovery_sessions`** - Tracking de sesiones

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `id` | TEXT PK | UUID de sesiÃ³n |
| `start_time` | DATETIME | Inicio |
| `end_time` | DATETIME | Fin (NULL si running) |
| `mode` | TEXT | shallow\|deep\|full |
| `max_depth` | INTEGER | Profundidad mÃ¡xima (0=unlimited) |
| `total_pages_visited` | INTEGER | PÃ¡ginas visitadas |
| `total_links_found` | INTEGER | Enlaces encontrados |
| `new_links` | INTEGER | Enlaces nuevos (no duplicados) |
| `duplicates_skipped` | INTEGER | Duplicados evitados |
| `status` | TEXT | running\|completed\|failed\|interrupted\|cancelled |
| `interrupted_at` | JSON | Estado para resume: `{"queue_size": 123, ...}` |
| `config_snapshot` | JSON | Config usada en esta sesiÃ³n |
| `errors` | INTEGER | Total de errores |

#### **`proxy_health`** - Salud de proxies

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `proxy_url` | TEXT PK | `http://ip:port` |
| `source` | TEXT | proxifly\|proxyscraper\|... |
| `protocol` | TEXT | http\|https\|socks4\|socks5 |
| `ip` | TEXT | IP address |
| `port` | INTEGER | Port |
| `country` | STRING(2) | CÃ³digo ISO paÃ­s |
| `anonymity` | TEXT | elite\|anonymous\|transparent |
| `total_requests` | INTEGER | Total requests realizados |
| `successful_requests` | INTEGER | Requests exitosos |
| `avg_response_time` | FLOAT | Tiempo promedio (segundos) |
| `score` | FLOAT | Score 0-100 (calculado) |
| `is_healthy` | BOOLEAN | Â¿Proxy usable? |
| `last_check` | DATETIME | Ãšltima validaciÃ³n |

---

## ğŸ›¡ï¸ Anti-Blocking Stack

### **1. Proxy Manager**

**Fuentes pÃºblicas automÃ¡ticas**:
- **Proxifly**: ~2,800 proxies (actualizaciÃ³n cada 5 min)
- **ProxyScrape**: ~6,500 proxies (actualizaciÃ³n cada 30 min)

**Funcionamiento**:
```python
# Cada request:
proxy = proxy_manager.get_next_proxy()  # Weighted random por score
response = make_request(url, proxy=proxy)
proxy_manager.mark_result(proxy, success=response.ok)
```

**Scoring** (0-100):
- 50% Success rate
- 25% Response time (<2s = 25pts, 2-5s = 15pts, >5s = 5pts)
- 15% Recency bonus (Ã©xito reciente)
- -20% Penalty por error reciente

**Auto-pruning**: Proxies con score <10 se descartan automÃ¡ticamente.

**Cache**: `data/proxies_cache.json` persistente entre ejecuciones.

---

### **2. Rate Limiter Adaptativo**

```python
# InicializaciÃ³n
limiter = AdaptiveRateLimiter(
    requests_per_minute=20,    # Base rate
    burst_size=5,              # Burst inicial
    backoff_on_429=True,       # Reducir al recibir 429
    decrease_factor=0.5,       # Reducir 50% en 429
    max_backoff_seconds=300    # Max backoff 5 minutos
)

# Uso
await limiter.wait()  # Bloquea hasta tener token

# Al recibir 429:
limiter.on_429()  # Reduce rate automÃ¡ticamente

# Tras Ã©xito:
limiter.on_success()  # Recupera rate gradualmente (10% por Ã©xito)
```

**Comportamiento**:
- Empieza a 20 req/min
- Si recibes 429 â†’ baja a 10 req/min, backoff 10s
- Siguientes 429 â†’ backoff exponencial (40s, 90s, 160s...)
- Cada request exitoso â†’ recupera 10% del rate perdido
- Rate nunca baja de 1 req/min

---

### **3. User-Agent Pool**

```python
ua_pool = UserAgentPool('config/user_agents.txt')
ua_pool.load()  # Lee 50+ UAs del archivo

# RotaciÃ³n por sesiÃ³n
ua_pool.set_session_ua()  # Elegir uno al azar al inicio

# O por request
ua = ua_pool.get_random()
```

**UAs incluidos** (ejemplos):
- Chrome 120 Windows/Mac/Linux
- Firefox 120 Windows/Linux
- Safari 17 macOS
- Edge 120 Windows
- Chrome Android, Safari iOS
- Opera, Brave

**ActualizaciÃ³n**: `python scripts/harvest_agents.py`

---

### **4. CAPTCHA Handler**

```python
captcha = CAPTCHAHandler(
    screenshots_dir='data/sessions/captchas',
    pause_on_captcha=True,
    auto_screenshot=True
)

# En cada pÃ¡gina:
should_skip = await captcha.should_skip_url(page, session_id)
if should_skip:
    continue  # Saltar esta URL
```

**DetecciÃ³n**:
- Patrones de texto (captcha, recaptcha, "verification", etc.)
- Selectores CSS especÃ­ficos (iframe[src*='recaptcha'])
- TÃ­tulos de pÃ¡gina
- Headers de Cloudflare

**Al detectar**:
1. ğŸ“¸ Screenshot automÃ¡tico
2. ğŸ“ Log con URL y timestamp
3. âš ï¸ Alerta visible en consola
4. â¸ï¸ Pausa esperando resoluciÃ³n manual
5. âœ… Continuar o saltar URL

---

## ğŸ“ˆ Rendimiento Estimado

### **Capacidad TeÃ³rica**

| MÃ©trica | Valor |
|---------|-------|
| Proxies en pool | 3,000+ (validados cada 6h) |
| User-Agents | 50+ (rotaciÃ³n por sesiÃ³n) |
| Rate limit por proxy | 20 req/min |
| **Req/min teÃ³rico** | **60,000** (3,000 Ã— 20) |
| **Req/min realista** | **15,000-25,000** (latencia, 429s, validaciones) |

### **Rendimiento por DÃ­a**

Asumiendo **20,000 requests/dÃ­a**:

```
PÃ¡ginas/dÃ­a:       500,000 - 800,000 (promedio 25ms/request)
Enlaces PDF/dÃ­a:   50,000 - 100,000 (10% de pÃ¡ginas contienen PDFs)
DB size/dÃ­a:       ~200-500 MB (metadata de enlaces)
```

### **Tiempo Total Estimado Cendoj**

- **PÃ¡ginas estimadas**: 500,000 - 1,000,000+
- **DuraciÃ³n**: **10-20 dÃ­as** (corriendo 24/7)
- **Enlaces PDF**: 50,000 - 100,000+

---

## ğŸ› Troubleshooting

### **Problema: No hay proxies disponibles**

```bash
# Ver estado
python cli.py proxies

# Si pool vacÃ­o, refrescar manualmente
python scripts/setup_proxies.py

# Ver logs
tail -f logs/cendoj_scraper.log
```

**SoluciÃ³n**: 
- Verificar conexiÃ³n a internet
- Las fuentes pÃºblicas pueden estar temporalmente down
- Revisar `data/proxies_cache.json`

---

### **Problema: Muchos 429/403**

```yaml
# En config/sites.yaml, reducir rate:
anti_blocking:
  rate_limiting:
    requests_per_minute: 10  # MÃ¡s conservador
    backoff_on_429: true
```

**SoluciÃ³n**:
1. Reducir `requests_per_minute`
2. Aumentar `burst_size` (para mÃ¡s espacio entre bursts)
3. El adaptive limiter deberÃ­a manejar esto automÃ¡ticamente

---

### **Problema: CAPTCHAs constantes**

```yaml
# Desactivar comportamientos "sospechosos"
anti_blocking:
  behavior:
    simulate_human: false  # Menos humano = mÃ¡s detectable?
  proxy:
    rotate_per_request: true  # Asegurar rotaciÃ³n
```

**SoluciÃ³n**:
- Los CAPTCHAs son seÃ±al de que estÃ¡s siendo detectado
- Revisar configuraciÃ³n de stealth
- Considerar agregar mÃ¡s delays
- Si aparecen, el handler pausa y permite resoluciÃ³n manual

---

### **Problema: Redis/DB lleno de duplicados**

```sql
-- Ver duplicados
SELECT normalized_url, COUNT(*) as cnt
FROM pdf_links
GROUP BY normalized_url
HAVING cnt > 1;

-- Limpiar (mantener el mÃ¡s reciente)
DELETE FROM pdf_links
WHERE id NOT IN (
    SELECT MAX(id) FROM pdf_links GROUP BY normalized_url
);
```

**Nota**: El sistema ya deduplica en tiempo real, pero si hay bugs, puedes limpiar manualmente.

---

### **Problema: InterrupciÃ³n y no resume**

```bash
# Ver sesiones interrumpidas
python cli.py sessions

# Reanudar explÃ­citamente
python cli.py discover --mode full --resume
```

**Nota**: La sesiÃ³n se guarda cada 100 pÃ¡ginas en `data/sessions/`

---

### **Problema: Proxies lentos (>10s)**

```bash
# Test individual
python scripts/test_proxies.py  # OpciÃ³n 1 (quick test)

# Ver scores en DB
SELECT proxy_url, avg_response_time, score
FROM proxy_health
WHERE is_healthy = 1
ORDER BY score DESC
LIMIT 20;
```

**SoluciÃ³n**: El sistema auto-prueba y descarta lentos. Si persisten, ajustar timeout en `config/sites.yaml` browser.timeout.

---

## ğŸ› ï¸ Desarrollo

### **Estructura del Proyecto**

```
cendoj/
â”œâ”€â”€ cli.py                    # CLI principal (Click)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py          # Config object con properties
â”‚   â”œâ”€â”€ sites.yaml           # Config YAML principal
â”‚   â””â”€â”€ user_agents.txt      # UA pool
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ discovery_scanner.py # Orquestador principal
â”‚   â”œâ”€â”€ deep_crawler.py      # BFS crawler
â”‚   â”œâ”€â”€ navigator.py         # Navegador (shallow mode)
â”‚   â”œâ”€â”€ browser.py           # BrowserManager con stealth
â”‚   â””â”€â”€ models.py            # Dataclasses (Sentence, etc.)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ proxy_manager.py     # GestiÃ³n de proxies
â”‚   â”œâ”€â”€ ua_pool.py           # Pool de UAs
â”‚   â”œâ”€â”€ adaptive_limiter.py  # Rate limiter adaptativo
â”‚   â”œâ”€â”€ behavior_simulator.py# SimulaciÃ³n humana
â”‚   â”œâ”€â”€ captcha_handler.py   # Manejo CAPTCHA
â”‚   â”œâ”€â”€ fingerprint.py       # Spoofing (existente)
â”‚   â”œâ”€â”€ rate_limiter.py      # Simple rate limiter (legacy)
â”‚   â””â”€â”€ logger.py            # Logging setup
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ database.py          # SQLAlchemy engine/session
â”‚   â””â”€â”€ schemas.py           # Modelos SQLAlchemy
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_proxies.py     # Inicializar pool
â”‚   â”œâ”€â”€ test_proxies.py      # Benchmark proxies
â”‚   â””â”€â”€ harvest_agents.py    # Actualizar UAs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sessions/            # .pkl de sesiones para resume
â”‚   â”œâ”€â”€ proxies_cache.json   # Cache de proxies
â”‚   â””â”€â”€ cendoj.db            # SQLite DB principal
â””â”€â”€ logs/
    â”œâ”€â”€ cendoj_scraper.log
    â””â”€â”€ discovery_YYYY-MM-DD.log
```

---

### **Workflow de Desarrollo**

```bash
# 1. Crear branch feature
git checkout -b feature/nueva-funcionalidad

# 2. Desarrollo
# ... modificar cÃ³digo ...

# 3. Test (solo syntax check por ahora)
python3 -m py_compile utils/nuevo_modulo.py

# 4. Commit
git add .
git commit -m "feat: descripciÃ³n clara"

# 5. Push y PR
git push origin feature/nueva-funcionalidad
# Abrir PR en GitHub
```

---

### **AÃ±adir Nuevo MÃ©todo de ExtracciÃ³n**

Ejemplo: agregar extracciÃ³n desde sitemap.xml

1. **Config**: AÃ±adir en `config/sites.yaml`:
   ```yaml
   sitemap:
     enabled: true
     urls:
       - "https://www.cendoj.es/sitemap.xml"
   ```

2. **CÃ³digo**: En `scraper/discovery_scanner.py` o crear `scraper/sitemap_parser.py`:
   ```python
   async def parse_sitemap(self, url):
       # Fetch sitemap
       # Extraer URLs
       # Filtrar .pdf
       # Retornar lista
   ```

3. **IntegraciÃ³n**: En `DiscoveryScanner._get_seed_urls()`:
   ```python
   if self.config.sitemap_enabled:
       sitemap_urls = await parse_sitemap()
       seed_urls.extend(sitemap_urls)
   ```

---

## ğŸ“š API Reference (Python)

### **DiscoveryScanner**

```python
from scraper.discovery_scanner import DiscoveryScanner
from config.settings import Config

config = Config()
scanner = DiscoveryScanner(config)

# Inicializar
await scanner.initialize()

# Run (async generator)
async for pdf in scanner.run():
    print(pdf['url'], pdf.get('validation'))

# Cleanup
await scanner.cleanup()
```

### **ProxyManager**

```python
from utils.proxy_manager import ProxyManager

pm = ProxyManager({'min_proxies_required': 100})
await pm.initialize()

proxy = pm.get_next_proxy('weighted')  # weighted|round_robin|random|best

# Marcar resultado
pm.mark_result(proxy, success=True, response_time=1.23)
# o
pm.mark_result(proxy, success=False, error="Timeout")

# Stats
stats = pm.get_stats()
```

### **DeepCrawler**

```python
from scraper.deep_crawler import DeepCrawler

crawler = DeepCrawler(
    browser_manager=browser,
    config=config,
    proxy_manager=pm,
    ua_pool=ua_pool,
    rate_limiter=limiter
)

await crawler.initialize(session_id='uuid', seed_urls=['https://...'])
async for pdf in crawler.crawl():
    process(pdf)
```

---

## ğŸ¤ Contributing

### **Guidelines**

1. **Fork** el repositorio
2. **Branch**: `git checkout -b feature/mi-feature`
3. **Code** siguiendo PEP8, type hints, docstrings
4. **Test**: Al menos comprobaciÃ³n de sintaxis
5. **Commit**: Mensajes claros, Conventional Commits
6. **PR**: DescripciÃ³n detallada, screenshots si es UI

### **Ãreas de ContribuciÃ³n**

- **OptimizaciÃ³n de discovery**: Mejores selectores para Cendoj
- **Anti-blocking**: Nuevas fuentes de proxies, mejor fingerprinting
- **UI/UX**: Dashboard web en tiempo real
- **Tests**: Unit y integration tests
- **DocumentaciÃ³n**: Ejemplos, guÃ­as especÃ­ficas
- **Performance**: Parallel crawling, async optimizations

---

## âš–ï¸ Legal Considerations

### **Uso Permitido**

Este proyecto estÃ¡ diseÃ±ado para:

- âœ… **InvestigaciÃ³n acadÃ©mica** en derecho y jurisprudencia
- âœ… **AnÃ¡lisis legal** y estudios de sentencias
- âœ… **Iniciativas de justicia abierta**
- âœ… **Archivo pÃºblico** de documentos legales

### **Restricciones**

- âŒ **Uso comercial** sin permiso expreso
- âŒ **RedistribuciÃ³n** de PDFs con derechos de autor
- âŒ **Scraping agresivo** (respetar rate limits)
- âŒ **EvasiÃ³n de medidas de seguridad** deliberada

### **Responsabilidad**

El usuario es responsable de:

- Cumplir la **ley espaÃ±ola** sobre documentos pÃºblicos
- Respetar los **tÃ©rminos de servicio** del Cendoj
- **No sobrecargar** los servidores (usa rate limiting!)
- Verificar **restricciones de copyright** antes de usar datos

Este proyecto no estÃ¡ afiliado con el Poder Judicial espaÃ±ol ni el Cendoj.

---

## ğŸ“„ License

MIT License - ver [LICENSE](LICENSE) para detalles.

---

## ğŸ™ Acknowledgments

- **InspiraciÃ³n**: Comunidad de web scraping y openness
- **Proxies pÃºblicos**: Proxifly, ProxyScrape
- **LibrerÃ­as**: Playwright, SQLAlchemy, Tenacity, Click
- **Legal**: Poder Judicial de EspaÃ±a por hacer pÃºblicos estos documentos

---

## ğŸ“ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/spidey000/cendoj-scraper/issues)
- **Discusiones**: [GitHub Discussions](https://github.com/spidey000/cendoj-scraper/discussions)
- **Email**: spidey00@gmail.com (soporte limitado)

---

**â­ Si este proyecto te es Ãºtil, considera darle una estrella en GitHub!**

**Ãšltima actualizaciÃ³n**: Febrero 2025 (v2.0 - Massive Discovery System)
